import os
import requests
import pandas as pd
from bs4 import BeautifulSoup
from bs4.element import Comment
import time
import traceback
import re 
import random
import urllib3
urllib3.PoolManager(maxsize=10)

# Base URL and save path
base_url = "https://www.pro-football-reference.com"
save_path = "C:\\NFLStats\\data"

def make_request_with_retry(url, headers, max_retries=3, timeout=30):
    """Make HTTP request with retry logic"""
    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=headers, timeout=timeout)
            response.raise_for_status()
            return response
        except requests.exceptions.Timeout:
            if attempt == max_retries - 1:
                print(f"Failed to fetch {url} after {max_retries} attempts due to timeout")
                raise
            print(f"Timeout occurred. Retrying... (Attempt {attempt + 1} of {max_retries})")
            time.sleep(5 * (attempt + 1))
        except requests.exceptions.RequestException as e:
            if attempt == max_retries - 1:
                print(f"Failed to fetch {url} after {max_retries} attempts: {str(e)}")
                raise
            print(f"Error occurred: {str(e)}. Retrying... (Attempt {attempt + 1} of {max_retries})")
            time.sleep(5 * (attempt + 1))

def safe_float_convert(cell):
    """Safely convert cell text to float."""
    if cell is None:
        return None
    try:
        text = cell.text.strip()
        return float(text) if text.replace('.', '', 1).isdigit() else None
    except (ValueError, AttributeError):
        return None
    
def process_drive_details(table, away_team, home_team):
    """Process drive details from play by play table including EPB and EPA with full diagnostic logging"""
    drive_details = []
    print("Processing play-by-play table...")
    
    if table:
        current_quarter = '1'
        rows = table.find_all('tr')
        print(f"Found {len(rows)} rows in play-by-play table")
        
        for i, row in enumerate(rows):
            # Skip header rows or rows without data
            if not row.find_all(['td', 'th']) or 'thead' in row.get('class', []):
                continue
                
            # Handle quarter changes
            quarter_cell = row.find('th', {'data-stat': 'quarter'})
            if quarter_cell and quarter_cell.text.strip().isdigit():
                current_quarter = quarter_cell.text.strip()
            
            # Get all relevant cells
            cells = {
                'time': row.find('td', {'data-stat': 'qtr_time_remain'}),
                'down': row.find('td', {'data-stat': 'down'}),
                'togo': row.find('td', {'data-stat': 'yds_to_go'}),
                'location': row.find('td', {'data-stat': 'location'}),
                'detail': row.find('td', {'data-stat': 'detail'}),
                'epb': row.find('td', {'data-stat': 'exp_pts_before'}),
                'epa': row.find('td', {'data-stat': 'exp_pts_after'})
            }
            
            # Only process rows with time and detail
            if cells['time'] and cells['detail']:
                # Get the time, handling potential links
                time_text = cells['time'].text.strip()
                if cells['time'].find('a'):
                    time_text = cells['time'].find('a').text.strip()

                # Log cell content before attempting conversion
                epb_raw = cells['epb'].text.strip() if cells['epb'] else 'None'
                epa_raw = cells['epa'].text.strip() if cells['epa'] else 'None'
                print(f"Row {i}: EPB raw content: '{epb_raw}'")
                print(f"Row {i}: EPA raw content: '{epa_raw}'")

                try:
                    # Use safe conversion for EPB and EPA values
                    epb_value = safe_float_convert(cells['epb'])
                    epa_value = safe_float_convert(cells['epa'])
                    
                    # Log converted values
                    print(f"Row {i}: EPB after conversion: {epb_value}")
                    print(f"Row {i}: EPA after conversion: {epa_value}")
                except Exception as e:
                    print(f"Row {i}: Error converting EPB/EPA values - {e}")
                    continue  # Skip this row if conversion fails
                
                # Create the basic detail dictionary
                detail = {
                    'Quarter': current_quarter,
                    'Time': time_text,
                    'Down': cells['down'].text.strip() if cells['down'] else None,
                    'ToGo': cells['togo'].text.strip() if cells['togo'] else None,
                    'Location': cells['location'].text.strip() if cells['location'] else None,
                    'Detail': cells['detail'].get_text(separator=' ').strip(),
                    'EPB': epb_value,
                    'EPA': epa_value
                }
                
                # Add parsed play information
                if detail['Detail']:
                    play_info = parse_play_details(detail['Detail'])
                    detail.update(play_info)
                    print(f"Updated detail: {detail}")
                
                drive_details.append(detail)
                print(f"Added play: Q{detail['Quarter']} - {detail['Time']} - {detail['Detail'][:50]}...")
    
    print(f"Processed {len(drive_details)} plays")
    return drive_details

def scrape_game_details(main_page_soup, box_score_soup, date_text, season, week, away_team, home_team):
    """Scrape game details including drive information"""
    print(f"\nProcessing game: {away_team} vs {home_team}")
    
    game_info = {
        'Date': date_text,
        'Season': season,
        'Week': week,
        'Away Team': away_team,
        'Home Team': home_team,
        'Game_Time': None
    }
    
    # Get game time
    scorebox_meta = box_score_soup.find('div', class_='scorebox_meta')
    if scorebox_meta:
        start_time_div = scorebox_meta.find(string=lambda text: 'Start Time' in text if text else False)
        if start_time_div:
            time_text = start_time_div.find_next(string=True)
            if time_text:
                cleaned_time = time_text.strip().replace(': ', '')
                game_info['Game_Time'] = cleaned_time
                print(f"Found game time: {cleaned_time}")
    
    # Find play-by-play div and table
    play_by_play_div = box_score_soup.find('div', id='div_pbp')
    if play_by_play_div:
        # Try to find the table directly first
        pbp_table = play_by_play_div.find('table', {'id': 'pbp'})
        
        # If not found directly, look in comments
        if not pbp_table:
            comments = play_by_play_div.find_all(string=lambda text: isinstance(text, Comment))
            for comment in comments:
                if 'table' in comment.lower() and 'pbp' in comment.lower():
                    comment_soup = BeautifulSoup(comment, 'html.parser')
                    pbp_table = comment_soup.find('table', {'id': 'pbp'})
                    if pbp_table:
                        break
        
        if pbp_table:
            print("Found play-by-play table")
            drive_details = process_drive_details(pbp_table, away_team, home_team)
            if drive_details:
                drive_df = pd.DataFrame(drive_details)
                
                # Add game info to each row
                for key, value in game_info.items():
                    drive_df[key] = value
                
                # Reorder columns
                columns_order = [
                    'Date', 'Season', 'Week', 'Away Team', 'Home Team', 'Game_Time',
                    'Quarter', 'Time', 'Down', 'ToGo', 'Location', 'Detail',
                    'EPB', 'EPA'
                ]
                
                # Ensure all columns exist
                for col in columns_order:
                    if col not in drive_df.columns:
                        drive_df[col] = None
                
                drive_df = drive_df[columns_order]
                print(f"Successfully processed {len(drive_df)} plays")
                return drive_df
            else:
                print("No drive details found in table")
        else:
            print("Could not find play-by-play table")
    else:
        print("Could not find play-by-play div")
    
    return pd.DataFrame()

def extract_commented_html(soup, div_id):
    """Extract HTML from comments within a div"""
    div = soup.find('div', id=div_id)
    if not div:
        return None
        
    # Find comments in the div
    from bs4.element import Comment
    comments = div.find_all(string=lambda text: isinstance(text, Comment))
    
    # Parse the first comment that contains a table
    for comment in comments:
        if 'table' in comment.lower():
            comment_soup = BeautifulSoup(comment, 'html.parser')
            return comment_soup
            
    return None

def scrape_game_details(main_page_soup, box_score_soup, date_text, season, week, away_team, home_team):
    """Scrape game details including drive information"""
    print(f"\nProcessing game: {away_team} vs {home_team}")
    
    game_info = {
        'Date': date_text,
        'Season': season,
        'Week': week,
        'Away Team': away_team,
        'Home Team': home_team,
        'Game_Time': None
    }
    
    # Get game time
    scorebox_meta = box_score_soup.find('div', class_='scorebox_meta')
    if scorebox_meta:
        start_time_div = scorebox_meta.find(string=lambda text: 'Start Time' in text if text else False)
        if start_time_div:
            time_text = start_time_div.find_next(string=True)
            if time_text:
                cleaned_time = time_text.strip().replace(': ', '')
                game_info['Game_Time'] = cleaned_time
                print(f"Found game time: {cleaned_time}")
    
    # Extract play-by-play data from comments
    comment_soup = extract_commented_html(box_score_soup, 'all_pbp')
    
    if comment_soup:
        print("Found play-by-play data in comments")
        # Find the table in the commented HTML
        pbp_table = comment_soup.find('table', {'id': 'pbp'})
        
        if pbp_table:
            print("Found play-by-play table")
            all_plays = []
            
            # Process each play
            rows = pbp_table.find_all('tr')
            print(f"Found {len(rows)} rows in play-by-play table")
            
            for row in rows:
                # Skip header rows
                if not row.find_all(['td', 'th']) or 'thead' in row.get('class', []):
                    continue
                
                # Get play details
                detail_cell = row.find('td', {'data-stat': 'detail'})
                if detail_cell:
                    detail_text = detail_cell.get_text(separator=' ').strip()
                    if detail_text:
                        # Parse the play details
                        play_info = parse_play_details(detail_text)
                        
                        # Add game info
                        play_info.update(game_info)
                        
                        # Get other play information
                        play_info['Quarter'] = row.find('th', {'data-stat': 'quarter'}).text.strip() if row.find('th', {'data-stat': 'quarter'}) else None
                        play_info['Time'] = row.find('td', {'data-stat': 'qtr_time_remain'}).text.strip() if row.find('td', {'data-stat': 'qtr_time_remain'}) else None
                        play_info['Down'] = row.find('td', {'data-stat': 'down'}).text.strip() if row.find('td', {'data-stat': 'down'}) else None
                        play_info['ToGo'] = row.find('td', {'data-stat': 'yds_to_go'}).text.strip() if row.find('td', {'data-stat': 'yds_to_go'}) else None
                        play_info['Location'] = row.find('td', {'data-stat': 'location'}).text.strip() if row.find('td', {'data-stat': 'location'}) else None
                        play_info['Detail'] = detail_text
                        
                        # Get EPB and EPA
                        epb_cell = row.find('td', {'data-stat': 'exp_pts_before'})
                        epa_cell = row.find('td', {'data-stat': 'exp_pts_after'})
                        
                        try:
                            play_info['EPB'] = float(epb_cell.text.strip()) if epb_cell and epb_cell.text.strip() else None
                            play_info['EPA'] = float(epa_cell.text.strip()) if epa_cell and epa_cell.text.strip() else None
                        except (ValueError, AttributeError):
                            play_info['EPB'] = None
                            play_info['EPA'] = None
                        
                        all_plays.append(play_info)
            
            if all_plays:
                # Create DataFrame
                drive_df = pd.DataFrame(all_plays)
                
                # Define column order
                columns_order = [
                    'Date', 'Season', 'Week', 'Away Team', 'Home Team', 'Game_Time',
                    'Quarter', 'Time', 'Down', 'ToGo', 'Location', 'Detail',
                    'Play_Type', 'Primary_Player', 'Receiver', 'Sack_By', 
                    'Run_Location', 'Run_Gap', 'Pass_Type', 'Pass_Location', 
                    'Pass_Yards', 'Field_Goal_Yards', 'Yards', 'Tackler', 'Result',
                    'Penalized_Player', 'Penalty_Yards', 'EPB', 'EPA'
                ]
                
                # Ensure all columns exist
                for col in columns_order:
                    if col not in drive_df.columns:
                        drive_df[col] = None
                
                # Reorder columns
                drive_df = drive_df[columns_order]
                
                print(f"Successfully processed {len(drive_df)} plays")
                
                # Debug information
                print("\nColumns in DataFrame:")
                for col in drive_df.columns:
                    non_null = drive_df[col].count()
                    total = len(drive_df)
                    print(f"{col}: {non_null}/{total} non-null values")
                
                return drive_df
            else:
                print("No plays found in table")
        else:
            print("Could not find play-by-play table in comments")
    else:
        print("No commented play-by-play data found")
    
    return pd.DataFrame()

def parse_play_details(detail_text):
    """Parse play details into structured categories with improved pattern matching."""
    print(f"Parsing detail: {detail_text}")
    
    play_info = {
        'Play_Type': None,
        'Primary_Player': None,
        'Receiver': None,
        'Sack_By': None,
        'Run_Location': None,
        'Run_Gap': None,
        'Pass_Type': None,
        'Pass_Location': None,
        'Pass_Yards': None,
        'Yards': None,
        'Tackler': None,
        'Result': None,
        'Penalized_Player': None,
        'Penalty_Yards': None,
        'Field_Goal_Yards': None,
        'Detail': detail_text
    }

    if not detail_text:
        return play_info

    # Clean and normalize the text
    detail_text = ' '.join(detail_text.lower().split())

    # Player name pattern
    player_pattern = r'(?:[A-Z][A-Za-z\'\.-]*\s*){1,4}'

    # Sack Pattern
    sacker_patterns = [
        fr'sacked by and\s+({player_pattern})\s*(?=(?:\s+for|\s+is|\s+\(|$|\.))',  # "sacked by and" pattern
        fr'sacked by\s+({player_pattern})\s*(?=(?:\s+for|\s+is|\s+\(|$|\.))'       # regular "sacked by" pattern
    ]

    # Extract yards first
    yards_patterns = [
        r'for (\-?\d+) yards?',
        r'(\-?\d+) yard (gain|loss)',
        r'loses (\-?\d+) yards?',
        r'gains (\d+) yards?',
        r'punts (\d+) yards'
    ]
    for pattern in yards_patterns:
        yards_match = re.search(pattern, detail_text)
        if yards_match:
            yards = int(yards_match.group(1))
            play_info['Yards'] = yards
            break

    # First determine play type and extract primary player
    if 'penalty' in detail_text:
        play_info['Play_Type'] = 'Penalty'
        penalty_match = re.search(fr'penalty on\s+({player_pattern})', detail_text, re.IGNORECASE)
        if penalty_match:
            play_info['Penalized_Player'] = penalty_match.group(1).strip().title()
        penalty_yards = re.search(r'(\d+) yards', detail_text)
        if penalty_yards:
            play_info['Penalty_Yards'] = int(penalty_yards.group(1))

    elif 'field goal' in detail_text:
        play_info['Play_Type'] = 'Field Goal'
        kicker_match = re.search(fr'^({player_pattern})\s+(?=\d+)', detail_text, re.IGNORECASE)
        if kicker_match:
            play_info['Primary_Player'] = kicker_match.group(1).strip().title()
        
        fg_yards_match = re.search(r'(\d+)\s+yard field goal', detail_text)
        if fg_yards_match:
            play_info['Field_Goal_Yards'] = int(fg_yards_match.group(1))
        
        if 'good' in detail_text:
            play_info['Result'] = 'Kick Good'
        else:
            play_info['Result'] = 'Missed Kick'

    elif 'kicks extra point' in detail_text:
        play_info['Play_Type'] = 'Extra Point'
        kicker_match = re.search(fr'^({player_pattern})\s+(?=kicks)', detail_text, re.IGNORECASE)
        if kicker_match:
            play_info['Primary_Player'] = kicker_match.group(1).strip().title()
        
        if 'good' in detail_text:
            play_info['Result'] = 'Kick Good'
        else:
            play_info['Result'] = 'Missed Kick'

    elif 'kicks off' in detail_text:
        play_info['Play_Type'] = 'Kickoff'
        kicker_match = re.search(fr'^({player_pattern})\s+(?=kicks)', detail_text, re.IGNORECASE)
        if kicker_match:
            play_info['Primary_Player'] = kicker_match.group(1).strip().title()

    elif 'scrambles' in detail_text:
        play_info['Play_Type'] = 'Run'
        scrambler_match = re.search(fr'^({player_pattern})\s+(?=scrambles)', detail_text, re.IGNORECASE)
        if scrambler_match:
            play_info['Primary_Player'] = scrambler_match.group(1).strip().title()

        if ' right end' in detail_text:
            play_info['Run_Location'] = 'Right'
            play_info['Run_Gap'] = 'End'
        elif ' left end' in detail_text:
            play_info['Run_Location'] = 'Left'
            play_info['Run_Gap'] = 'End'
        elif ' right guard' in detail_text:
            play_info['Run_Location'] = 'Right'
            play_info['Run_Gap'] = 'Guard'
        elif ' left guard' in detail_text:
            play_info['Run_Location'] = 'Left'
            play_info['Run_Gap'] = 'Guard'
        elif ' right tackle' in detail_text:
            play_info['Run_Location'] = 'Right'
            play_info['Run_Gap'] = 'Tackle'
        elif ' left tackle' in detail_text:
            play_info['Run_Location'] = 'Left'
            play_info['Run_Gap'] = 'Tackle'
        elif ' right' in detail_text:
            play_info['Run_Location'] = 'Right'
        elif ' left' in detail_text:
            play_info['Run_Location'] = 'Left'
        elif ' middle' in detail_text:
            play_info['Run_Location'] = 'Middle'

        if play_info['Yards'] is not None:
            if play_info['Yards'] > 0:
                play_info['Result'] = 'Gain'
            elif play_info['Yards'] < 0:
                play_info['Result'] = 'Loss'
            else:
                play_info['Result'] = 'No Gain'

    elif 'sacked' in detail_text:
        play_info['Play_Type'] = 'Pass'
        qb_match = re.search(fr'^({player_pattern})\s+(?=sacked)', detail_text, re.IGNORECASE)
        if qb_match:
            play_info['Primary_Player'] = qb_match.group(1).strip().title()

        for pattern in sacker_patterns:
            sacker_match = re.search(pattern, detail_text, re.IGNORECASE)
            if sacker_match:
                play_info['Sack_By'] = sacker_match.group(1).strip().title()
                break
        
        play_info['Result'] = 'Sack'

    elif 'pass' in detail_text:
        play_info['Play_Type'] = 'Pass'
        qb_match = re.search(fr'^({player_pattern})\s+(?=pass)', detail_text, re.IGNORECASE)
        if qb_match:
            play_info['Primary_Player'] = qb_match.group(1).strip().title()
        
        receiver_patterns = [
            fr'(?:intended for|complete to|incomplete to|to)\s+({player_pattern})\s*(?=(?:\s+for|\s+is|\s+\(|$|\.))',
        ]
        for pattern in receiver_patterns:
            receiver_match = re.search(pattern, detail_text, re.IGNORECASE)
            if receiver_match:
                play_info['Receiver'] = receiver_match.group(1).strip().title()
                break

        if 'short' in detail_text:
            play_info['Pass_Type'] = 'Short'
        elif 'deep' in detail_text:
            play_info['Pass_Type'] = 'Deep'

        if ' left' in detail_text:
            play_info['Pass_Location'] = 'Left'
        elif ' right' in detail_text:
            play_info['Pass_Location'] = 'Right'
        elif ' middle' in detail_text:
            play_info['Pass_Location'] = 'Middle'

        if 'incomplete' in detail_text:
            play_info['Result'] = 'Incomplete'
        elif 'intercepted' in detail_text:
            play_info['Result'] = 'Interception'
        elif 'complete' in detail_text:
            play_info['Result'] = 'Complete'
            if play_info['Yards'] is not None:
                play_info['Pass_Yards'] = play_info['Yards']

    # Handles Kicks and Punts
    elif 'kicks off' in detail_text or 'kicks onside' in detail_text:
        play_info['Play_Type'] = 'Kickoff'
        kicker_match = re.search(fr'^({player_pattern})\s+(?=kicks)', detail_text, re.IGNORECASE)
        if kicker_match:
            play_info['Primary_Player'] = kicker_match.group(1).strip().title()
        
        returner_match = re.search(fr'recovered by\s+({player_pattern})', detail_text, re.IGNORECASE)
        if returner_match:
            play_info['Receiver'] = returner_match.group(1).strip().title()

    elif 'punts' in detail_text:
        play_info['Play_Type'] = 'Punt'
        punter_match = re.search(fr'^({player_pattern})\s+(?=punts)', detail_text, re.IGNORECASE)
        if punter_match:
            play_info['Primary_Player'] = punter_match.group(1).strip().title()
        
        returner_match = re.search(fr'returned by\s+({player_pattern})\s*(?=(?:\s+for|\s+is|\s+\(|$|\.))', detail_text, re.IGNORECASE)
        if returner_match:
            play_info['Receiver'] = returner_match.group(1).strip().title()

    # Handles Run
    elif any(x in detail_text for x in [
        'left end', 'right end', 'left guard', 'right guard', 
        'left tackle', 'right tackle', 'up the middle', 'middle for',
        # Add these simple patterns
        ' for ', ' for no gain', ' for touchdown'
    ]) and 'pass' not in detail_text:  # Make sure it's not a pass play
        play_info['Play_Type'] = 'Run'
        
        # Single strict pattern for runner name
        parts = re.split(r'\s+(for|left|right|middle|up)', detail_text, maxsplit=1)
        if parts and len(parts) > 0:
            play_info['Primary_Player'] = parts[0].strip().title()
                
        # Location and gap detection
        if ' right end' in detail_text:
            play_info['Run_Location'] = 'Right'
            play_info['Run_Gap'] = 'End'
        elif ' left end' in detail_text:
            play_info['Run_Location'] = 'Left'
            play_info['Run_Gap'] = 'End'
        elif ' right guard' in detail_text:
            play_info['Run_Location'] = 'Right'
            play_info['Run_Gap'] = 'Guard'
        elif ' left guard' in detail_text:
            play_info['Run_Location'] = 'Left'
            play_info['Run_Gap'] = 'Guard'
        elif ' right tackle' in detail_text:
            play_info['Run_Location'] = 'Right'
            play_info['Run_Gap'] = 'Tackle'
        elif ' left tackle' in detail_text:
            play_info['Run_Location'] = 'Left'
            play_info['Run_Gap'] = 'Tackle'
        elif ' middle' in detail_text:
            play_info['Run_Location'] = 'Middle'
            play_info['Run_Gap'] = 'Middle'
        elif ' up the middle' in detail_text:
            play_info['Run_Location'] = 'Middle'
            play_info['Run_Gap'] = 'Middle'

        # Set result based on yards
        if play_info['Yards'] is not None:
            if play_info['Yards'] > 0:
                play_info['Result'] = 'Gain'
            elif play_info['Yards'] < 0:
                play_info['Result'] = 'Loss'
            else:
                play_info['Result'] = 'No Gain'

    # Extract tacklers for any play type
    tackler_match = re.search(fr'tackle by\s+({player_pattern}(?:\s+and\s+{player_pattern})?)', detail_text, re.IGNORECASE)
    if tackler_match:
        tacklers = [name.strip().title() for name in tackler_match.group(1).split(' and ')]
        play_info['Tackler'] = tacklers

    # Override result for touchdowns and fumbles
    if 'touchdown' in detail_text:
        play_info['Result'] = 'Touchdown'
    elif 'fumble' in detail_text and not play_info['Result']:
        play_info['Result'] = 'Fumble'

    print(f"Parsed play info: {play_info}")
    return play_info

def make_request_with_retry(url, headers, max_retries=3, base_delay=10):  # Reduced retries and base delay
    session = requests.Session()
    
    # Configure your purchased proxy details
    proxy_ip = "176.103.228.214"
    proxy_port = "46212"
    proxy_user = "8u1Xqoto81EPud1"
    proxy_pass = "vfJnJWuZr0LhU3q"
    
    # Set up proxy with authentication
    proxy_auth = f"{proxy_user}:{proxy_pass}@"
    proxies = {
        'http': f'http://{proxy_auth}{proxy_ip}:{proxy_port}',
        'https': f'http://{proxy_auth}{proxy_ip}:{proxy_port}'
    }
    
    # Streamlined headers - keep only essential ones
    default_headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Connection': 'keep-alive'
    }
    
    # Use session.mount to set max retries for both http and https
    adapter = requests.adapters.HTTPAdapter(max_retries=1)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    
    for attempt in range(max_retries):
        try:
            # Reduced delay calculation
            delay = base_delay * (attempt + 1) + random.uniform(1, 3)
            if attempt > 0:
                print(f"Waiting {delay:.1f} seconds before retry...")
                time.sleep(delay)
            
            print(f"Making request through proxy: {proxy_ip}:{proxy_port}")
            response = session.get(
                url, 
                headers=default_headers,
                proxies=proxies,
                timeout=15,  # Increased timeout slightly for proxy
                verify=True
            )
            response.raise_for_status()
            
            # Reduced success delay
            time.sleep(base_delay)
            return response
            
        except requests.exceptions.RequestException as e:
            print(f"Error occurred: {str(e)}")
            if attempt == max_retries - 1:
                raise
            print(f"Retrying... (Attempt {attempt + 1} of {max_retries})")

def scrape_nfl_data(season, week):
    week_url = f"{base_url}/years/{season}/week_{week}.htm"
    week_path = os.path.join(save_path, str(season), f"Week_{week}")
    os.makedirs(week_path, exist_ok=True)
    
    try:
        print(f"Starting scrape for Week {week}...")
        time.sleep(5)
        
        response = make_request_with_retry(week_url, {})
        main_page_soup = BeautifulSoup(response.text, 'lxml')
        
        all_drive_details = []
        
        games = main_page_soup.find_all('div', class_='game_summary')
        if not games:
            print(f"No games found for Season {season}, Week {week}")
            return
            
        for game_index, game in enumerate(games):
            try:
                print(f"Processing game {game_index + 1} of {len(games)}")
                
                # Improved error handling for game data extraction
                boxscore_link = game.find('td', class_='right gamelink')
                if not boxscore_link or not boxscore_link.find('a'):
                    print(f"No boxscore link found for game {game_index + 1}")
                    continue
                    
                full_boxscore_url = base_url + boxscore_link.find('a')['href']
                
                # Add more descriptive logging
                print(f"Fetching boxscore from: {full_boxscore_url}")
                
                box_score_response = make_request_with_retry(full_boxscore_url, {})
                box_score_soup = BeautifulSoup(box_score_response.text, 'lxml')
                
                team_rows = game.find_all('tr', class_=['winner', 'loser'])
                if len(team_rows) < 2:
                    print(f"Invalid team data for game {game_index + 1}")
                    continue
                    
                away_team = team_rows[0].find('td')
                home_team = team_rows[1].find('td')
                
                if not away_team or not home_team:
                    print(f"Missing team data for game {game_index + 1}")
                    continue
                    
                away_team = away_team.text.strip()
                home_team = home_team.text.strip()
                
                date_row = game.find('tr', class_='date')
                if not date_row or not date_row.find('td'):
                    print(f"Missing date for game {game_index + 1}")
                    continue
                
                game_date = date_row.find('td').text.strip()
                
                # Add diagnostic print before processing details
                print(f"Processing details for {away_team} vs {home_team} on {game_date}")
                
                drive_df = scrape_game_details(game, box_score_soup, game_date, season, week, away_team, home_team)
                
                if not drive_df.empty:
                    all_drive_details.append(drive_df)
                    print(f"Successfully processed {len(drive_df)} plays")
                else:
                    print(f"No plays found for game {game_index + 1}")
                
                if game_index < len(games) - 1:
                    time.sleep(3)
                    
            except Exception as e:
                print(f"Error processing game {game_index + 1}: {str(e)}")
                traceback.print_exc()  # Add stack trace for better debugging
                continue
        
        if all_drive_details:
            try:
                print("Combining all game data...")
                combined_df = pd.concat(all_drive_details, ignore_index=True)
                
                # Print shape and columns for debugging
                print(f"Combined DataFrame shape: {combined_df.shape}")
                print("Columns found:", combined_df.columns.tolist())
                
                columns_order = [
                    'Date', 'Season', 'Week', 'Away Team', 'Home Team', 'Game_Time',
                    'Quarter', 'Time', 'Down', 'ToGo', 'Location', 'Detail',
                    'Play_Type', 'Primary_Player', 'Receiver', 'Sack_By', 
                    'Run_Location', 'Run_Gap', 'Pass_Type', 'Pass_Location', 
                    'Pass_Yards', 'Field_Goal_Yards', 'Yards', 'Tackler', 'Result',
                    'Penalized_Player', 'Penalty_Yards', 'EPB', 'EPA'
                ]
                
                for col in columns_order:
                    if col not in combined_df.columns:
                        print(f"Adding missing column: {col}")
                        combined_df[col] = None
                
                combined_df = combined_df[columns_order]
                
                file_path = os.path.join(week_path, f"{season}_Week{week}_Drive_Details.xlsx")
                print(f"Saving data to {file_path}")
                combined_df.to_excel(file_path, index=False)
                print(f"Successfully saved {len(combined_df)} plays")
                
                # Print sample data for verification
                print("\nSample of saved data:")
                print(combined_df.head())
                
            except Exception as e:
                print(f"Error saving drive details: {e}")
                traceback.print_exc()
                
    except Exception as e:
        print(f"Failed to fetch week data for Season {season}, Week {week}: {str(e)}")
        traceback.print_exc()

# Additional helper function to clean the parsed data
def clean_parsed_data(df):
    """Clean and standardize the parsed data"""
    # Convert Tackler from list to string if necessary
    if 'Tackler' in df.columns:
        df['Tackler'] = df['Tackler'].apply(
            lambda x: ', '.join(x) if isinstance(x, list) else x
        )
    
    # Ensure numeric columns are properly typed
    numeric_columns = ['Yards', 'Pass_Yards', 'Penalty_Yards', 'EPB', 'EPA']
    for col in numeric_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    return df
def main():
    seasons = [2015]  # Modify as needed
    weeks = list(range(1, 2))  # Scrape all regular season weeks
    
    for season in seasons:
        for week in weeks:
            print(f"\nScraping data for Season {season}, Week {week}")
            scrape_nfl_data(season, week)
            time.sleep(5)  # Add delay between weeks

if __name__ == "__main__":
    main()